<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="YAO LI" >

    <link rel="shortcut icon" href="./YaoLi_files/img/unc_logo.svg">

    <title>Yao Li</title>

    <!-- Bootstrap Core CSS -->
    <link href="./YaoLi_files/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="./YaoLi_files/custom.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

  <style>
        .navbar
        {
            border:5px solid #002855;
        }
      </style>
      
    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation" style="background-color: #002855;">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>

		<a class="navbar-brand" href="https://liyao880.github.io/yaoli/#" style="color: #808080;"> 
			<h2>Yao Li</h2></a>
		<!--                <a class="navbar-brand" href="#"><img width="40px" height="40px" src="aaa.png" style="margin-top:5px;background:white;" alt=""></a> -->
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="https://liyao880.github.io/yaoli/#about" style="color: #808080; font-size: 20px;">Home</a></li>
                    <li><a href="https://liyao880.github.io/yaoli/publications_full.html" style="color: #808080;font-size: 20px;">Publications</a></li>
                    <li><a href="https://liyao880.github.io/yaoli/research.html" style="color: #808080;font-size: 20px;">Research</a></li>
                    <li><a href="https://liyao880.github.io/yaoli/teaching.html" style="color: #808080;font-size: 20px;">Teaching</a></li>
                    <li><a href="https://liyao880.github.io/yaoli/talks.html" style="color: #808080;font-size: 20px;">Talks</a></li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>


    <!-- Page Content -->
   	  <style>
    .image{
      text-align: center;
    }
    .image label{
      display: block;      
      line-height: 0; /*To stick the label under the bottom edge of the image*/
    }
    .image img{
      display: inline-block;
    }
  	</style> 
		
    <div class="container">

	    <section id="research">
        <a name="research"></a>
        <!-- Heading Row -->


      <h2>Research</h2>
		    
		<h3>AI Security</h3>    
		
		<h4>Adversarial Robustness</h4>  
	     <div class="page">
	 
      <div class="image">
      
       <img src="./YaoLi_files/img/bagel_adv.png">
       <br>
       <br>
      <label>Example of Adversarial Attack</label>
      </div>
        </div>
        
		    <br>
		     Studies have showed that machine learning models, not only neural networks, are vulnerable to adversarial examples. 
		     By adding imperceptible perturbations to the original inputs, the attacker can get adversarial examples to fool a learned classifier. 
		     Adversarial examples are indistinguishable from the original input image to human, but are misclassified by the classifier. 
		     To illustrate, consider the above bagel images. To humans, the two images appear to be the same -- the vision system of human will 
		     identify each image as a bagel. The image on the left side is an ordinary image of a bagel (the original image). 
		     However, the image on the right side is generated by adding a small and imperceptible perturbation that forces a particular classifier 
		     to classify it as a piano. Adversarial attacks are not limited to image classification models; 
		     they can also fool other types of machine learning models. 
		     Given the widespread application of machine learning models in various tasks, 
		     it is crucial to study the security issues posed by adversarial attacks.
		     
		    <br>
	     	<h5>Preprints and Publications of this direction:</h5>
		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						TART: Boosting Clean Accuracy Through Tangent Direction Guided Adversarial Training.
				</a></b> Bongsoo Yi, Rongjie Lai, and Yao Li.
						 <i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>	     	

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Can Machine Tell the Distortion Difference? A Reverse Engineering Study of Adversarial Attacks.
				</a></b> Xiawei Wang, Cho-Jui Hsieh, Thomas C. M. Lee, and Yao Li.
						 <i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>	
				
	
			<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://ieeexplore.ieee.org/document/10472606">
						Adversarial Examples Detection With Bayesian Neural Network.
				</a></b>
				Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee.
		<i> In IEEE TETCI, 2024.  </i>
				</p>
			<ul>
            </div>
			</div>

			<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://aclanthology.org/2022.emnlp-main.440.pdf">
						ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation. 
				</a></b>
				Fan Yin, Yao Li, Cho-Jui Hsieh, Kai-Wei Chang. 
		<i> In EMNLP, 2022.  
					</i>
				</p> 
			<ul>
            </div>
			</div>
	
				<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://www.tandfonline.com/doi/abs/10.1080/00031305.2021.2006781?journalCode=utas20">
						A Review of Adversarial Attack and Defense for Classification Methods. 
				</a></b>
				Yao Li, Minhao Cheng, Cho-Jui Hsieh, Thomas C. M. Lee. 
		<i> In The American Statistician, 2021.  
					</i>
				</p>  
			<ul>
            </div>
			</div>

				<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://ieeexplore.ieee.org/document/9710826">
						Towards Robustness of Deep Neural Networks via Regularization.
				</a></b>
				Yao Li, Martin Renqiang Min, Thomas C. M. Lee, Wenchao Yu, Erik Kruus, Wei Wang, Cho-Jui Hsieh. 
		<i> In ICCV, 2021. 
					</i>
				</p>
			<ul>
            </div>
			</div>

				<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://openreview.net/forum?id=rk4Qso0cKm">
						Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network.
				</a></b>
				Xuanqing Liu, Yao Li, Chongruo Wu, Cho-Jui Hsieh. 
		<i> In ICLR, 2019. 
					</i>
				</p> 
			<ul>
            </div>
			</div>
			
																
		<h4>Backdoor Learning</h4>  
	     <div class="page">
	 
      <div class="image">
      
       <img src="./YaoLi_files/img/back.png">
       <br>
       <br>
      <label>Example of Backdoor Attack</label>
      </div>
        </div>		
		
		<br>
		Artificial Intelligence (AI) has become one of the most exciting areas in recent years, as it has achieved state-of-the-art performance 
		and demonstrated fundamental breakthroughs in many challenging tasks. The Achillesâ€™ heel of the technology, however, is what makes 
		it possible -- learning from data. By poisoning the training data, adversaries can corrupt the model, causing severe or even drastic consequences. 
		 Backdoor attacks insert a hidden backdoor into a target model so that the model performs well on benign examples, 
		 but when a specific pattern, often known as a trigger, appears in the input data, the model will 
		 produce incorrect results, such as associating the trigger with a target label irrespective of what the true label is. 
		For example, the figure above gives a specific example of a backdoor attack on image data, 
		but our research is not limited to studying this type of backdoor attack.
		<br>
		<h5>Preprints and Publications of this direction:</h5>
		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Analysis of Invisible Textual Backdoor Attack.
				</a></b>	 Xinglin Li, Xianwen He, Yao Li, and Minhao Cheng.
						<i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>			

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Trusted Aggregation (TAG): Backdoor Defense in Federated Learning.
				</a></b>	 Joseph Daniel Lavond, Minhao Cheng, and Yao Li.
						<i>2024.</i>
			</p>
			<ul>
            </div>
			</div>	
				

		
		
		
		<h3>Computational Pathology</h3>    
	     	<br>
	     	
	     <div class="page">
	 
      <div class="image">
      
       <img src="./YaoLi_files/img/framework.png" width="900" >
       <br>
       <br>
      <label>Pipeline of developing a machine learning system for medical image analysis and challenges.</label>
      </div>
        </div>

			<br>		    		    
		    		    	    
		  Despite extensive research in applying machine learning methods to medical image analysis, 
		  many challenges in this field remain unsolved. The figure above illustrates the pipeline for developing 
		  a machine learning system for medical image analysis, highlighting the challenges at each stage. 
		  Our focus is on addressing the problems that arise at each step of this pipeline to enhance the utilization of medical images
			<br>
			<h5>Preprints and Publications of this direction:</h5>
		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Stain SAN: Simultaneous Augmentation and Normalization for Histopathology Images. 
				</a></b> Taebin Kim, Yao Li, Benjamin C. Calhoun, Aatish Thennavan, Lisa A. Carey, W. Fraser Symmans, Melissa A. Troester, Charles M. Perou, and J.S. Marron.
						<i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>			

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Grade-adjusted Image Analysis of Breast Cancer to Predict Subtype.
				</a></b>	 Dong Neuck Lee, Yao Li, Melissa A. Troester, Katherine A. Hoadley, Benjamin C. Calhoun, Charles M. Perou, Linnea T. Olsson, Alina M. Hamilton, Bentley R. Midkiff, and J.S. Marron.
						<i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Region of Interest Detection in Melanocytic Skin Tumor Whole Slide Images. 
				</a></b> Yi Cui, Yao Li, Jayson R. Miedema, Sherif Farag, Sharon N. Edmiston, J. S. Marron, and Nancy E. Thomas. 
						<i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Visual Intratumor Heterogeneity and Breast Tumor Progression.
				</a></b>	 Yao Li, Sarah C. Van Alsten, Dong Neuck Lee, Taebin Kim, Benjamin C. Calhoun, Charles M. Perou, Sara E. Wobker, J.S. Marron, Katherine A. Hoadley, and Melissa A. Troester.
						<i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Multiple Instance Learning for Breast Cancer Histopathology Images. 
				</a></b>	 Taebin Kim, Benjamin C. Calhoun, Yao Li, Aatish Thennavan, Lisa A. Carey, W. Fraser Symmans, Melissa A. Troester, Charles M. Perou, and J.S. Marron.
						<i>2024.</i>
			</p>
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://pubmed.ncbi.nlm.nih.gov/32497023/">
						L-Arginine Supplementation in Severe Asthma.
				</a></b>
				Shu-Yi Liao, Megan R Showalter, Angela L Linderholm, Lisa Franzi, Celeste Kivler , Yao Li, 
				Michael R Sa, Zachary A Kons, Oliver Fiehn, Lihong Qi, Amir A Zeki, Nicholas J Kenyon.
		<i> In JCI Insight, 2020. 
					</i>
				</p> 
			<ul>
            </div>
			</div>

	
																		
			<br>
	<h3>AI Efficiency</h3>  
	To apply machine learning to a real-world problem, we design a model (such as a deep neural network) for that problem, 
	train the model using training data, and then deploy the model in the application to interact with the real world. 
	There are many difficulties in this pipeline that restrict the applicability of machine learning. 
	First, designing machine learning models that can work with various types of data efficiently is a challenging task as one method may 
	work well on a certain type of data but may not be applicable to other types due to large scale or some other limitations.		
	
	<h5>Preprints and Publications of this direction:</h5>
		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Feddecay: Adapting to Data Heterogeneity in Federated Learning with Within-round Learning Rate Decay.
				</a></b>	 Joseph Daniel Lavond, Minhao Cheng, and Yao Li.
						<i>2024.</i>
			</p>
			<ul>
            </div>
			</div>
			
		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="">
						Dueling Bandits with Stochastic Delayed Feedback.
				</a></b>	 Bongsoo Yi, Yue Kang, and Yao Li.
						<i>Under Review, 2024.</i>
			</p>
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://arxiv.org/pdf/2211.11152.pdf">
						You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model. 
				</a></b>
				Shengkun Tang, Yaqing Wang, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, Yi Liang, Dongkuan Xu. 
		<i> In CVPR, 2023.  
					</i>
				</p> 
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://arxiv.org/pdf/2212.06152.pdf">
						Accelerating Dataset Distillation via Model Augmentation. 
				</a></b>
				Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, Dongkuan Xu. 
		<i> In CVPR, 2023.  
					</i>
				</p>  
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://www.tandfonline.com/doi/abs/10.1080/00401706.2019.1665591?casa_token=mTT7eWZoOEsAAAAA%3AA4dvuYgUUJp0T3VKGZs4hfeRDMSU1x-O9q5HhQvi7F5TVaxrazJ8Ulz4BQomL-zBL_9vDC-8nOdBfg&journalCode=utch20">
						Uncertainty Quantification for High-Dimensional Sparse Nonparametric Additive Models.
				</a></b>
				Qi Gao, Randy C. S. Lai, Thomas C. M. Lee, Yao Li. 
		<i> In Technometrics, 2019. 
					</i>
				</p>  
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://proceedings.neurips.cc/paper/2018/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf">
						Learning from Group Comparisons: Exploiting Higher Order Interactions.
				</a></b>
				Yao Li, Minhao Cheng, Kevin Fujii, Fushing Hsieh, Cho-Jui Hsieh. 
		<i> In NeurIPS, 2018. 
					</i>
				</p>   
			<ul>
            </div>
			</div>

		<div class="row">
            <div class="col-sm-11">
            <ul>
				<p><b><a href="https://proceedings.neurips.cc/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">
						Scalable Demand-aware Recommendation.
				</a></b>
				Jinfeng Yi, Cho-Jui Hsieh, Kush R. Varshney, Lijun Zhang, Yao Li. 
		<i> In NeurIPS, 2017. 
					</i>
				</p>    
			<ul>
            </div>
			</div>
			
																					
	<br>
        
        </section>
<br>
<br>
<br>
<br>
<br>
 
    <!-- custom -->
    <script src="./Cho-Jui Hsieh_files/custom.js"></script>

    <!-- jQuery -->
    <script src="./Cho-Jui Hsieh_files/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./Cho-Jui Hsieh_files/bootstrap.min.js"></script>




</div></body></html>
